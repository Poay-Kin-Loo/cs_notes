=====================================================================
 0. Table Of Contents
=====================================================================
 1. Overview
 2. Different Complexities
 3. Complexity Comparison
 4. Examples
 
=====================================================================
 1. Overview
=====================================================================

Big O notation is used to describe efficiency of algorithms
(time and space complexity) in relation to input size.

Big O notation is NOT about an algorithm being "quick" or "slow".
Rather it is about the rate of increase - how fast the time will
increase in proportion to the input size.

 Example
 -------
 The following for-loop has complexity O(n):

	for(i=0; i<n; i++){
		O(1) instruction1;
		O(1) instruction2;
	}

 The following loops are also O(n):
	
	// O(n) + O(n) = O(2n) = O(n)
	for(i=0; i<n; i++){
		O(1) instruction1;
	}
	
	for(i=0; i<n; i++){
		O(1) instruction2;
	}
	
 The single for-loop will run faster then the two separate ones
 because it does n iterations only once. Complexity is the same
 but not the run time.

 Example
 -------
 Insertion sort has complexity O(n^2) whereas quick sort - O(n*log n).
 O(n*log n) complexity is better than O(n^2). Yet, insertion sort is
 faster than quick sort for small input.
 
 Note: there are two sides to learning Big O:
 
	1. Know and understand what different complexities mean
	2. Understand how to calculate Big O for a piece of code

=====================================================================
 2. Different Complexities
=====================================================================

 * O(1) - constant complexity

   No matter how large is the input, it always takes one operation/
   will execute in the same time, e.g.
   
		print x
		x + y
		if (x < 5) { ... }
		
 * O(log n) - logarithmic complexity (assume log base is 2)
 
   If we double the input, only one extra operation/iteration will be
   done.
   
   In general, for O(log n) problems on each iteration we halve the
   dataset (and we end up doing it log n time only).
   
   Let's use binary search to find x=4 in:
   
		{1, 4, 5, 7, 8, 9, 11, 12, 15}
		
		0) start with the full list (n = 9, indices 0 to 8) 
		1) split the list in half (n = 9/2 = 4, indices 0 to 4)
		2) split the list in half (n = 4/2 = 2, indices 0 to 2)
		3) split the list in half (n = 2/2 = 1, indices 0 to 1)
		
	We split n 3 times to find x=4 in the list:
	
		log n = log 9 ~ 3   ==> O(log n)
		
	Note: if we were looking for 8, we would find it after the first
	split so "log 9 ~ 3" would not hold. But with big O in programming
	we look at the worst case which means an element that is at the end
	or beginning of our sample list (like 3).
		
 * O(n) - linear complexity
  
   The number of operations increases proportionally to the size of 
   input, e.g. double the size of input and the number of operations
   will double too.
   
   Good example of O(n) is a simple for-loop:
   
		for(i=0; i<n; i++){
			print i;
		}
	
	Initially, the loop runs n times. Then we increase the size of
	dataset by 5 elements (n+5) and the loop runs 5 more times. The
	increase is proportional to the size of input.
		
 * O(n*log n) - loglinear complexity
 
   This type of complexity usually happens when you need to do work of
   O(log n) for each record (and there is n records).
   
   A good example is merge sort algorithm where we split an unsorted
   list into n sublists, each containing one element. Then repeatedly
   we merge sublists to produce new sorted sublists until there is only
   one sublist remaining:

		O(log n) because we halve the list until we have 1 element lists
		O(n) because of the max number of comparisons on each level
		
 * O(n^2) - squared complexity
 
   The number of operations gets squared in relation to the size of input.
   
   A good example is a nested for-loop:
   
		for(i=0; i<n; i++) {
			for(j=0; j<n; j++) {
				print i,j;
			}
		}
		
	For each element we go through all elements. If n = 2, print will
	be executed four times - twice for i = 0 and twice for i = 1:
	
			0,0
			0,1
			1,0
			1,1
			
	If we change to n = 3, print will be executed... nine times (thrice
	for i = 0, thrice for i = 1 and thrice for i = 2).
	
 * O(2^n) - exponential complexity
 
   Every time an element is added to the input, the number of executions 
   doubles.
   
   A good example here is a recursive version of Fibonacci:
   
		fib(int n) {
			if n <= 2:
				return 1
			return fib(n-1) + fib(n-2)
		}
		
	For n = 2, our execution tree has only one node:
	
					fib(2)
					
	For n = 3, we add an extra level and double the number of executions: 
	
					fib(3)					
					/	\
				fib(2)  fib(1)				
				
	For n = 5, we add more two more levels (one not fully complete) and
	the	number of executions in general doubles per level:
	
					fib(5)					
					/	\
				fib(4)	fib(3)				
				/	|     |	  \
		  fib(3) fib(2) fib(2) fib(1)
          /    \
        fib(2)	fib(1)	  
			
 * O(n!) - factorial complexity
 
   By adding an extra element to input of size n, the number of
   executions increases n times.
   
   A good example is the travelling salesman problem where we need to
   consider all permutations on n-element dataset (=n!).
	
=====================================================================
 3. Complexity Comparison
=====================================================================

In general, O(1) is the best complexity. All the others compare as
follows:

	O(1) < O(log n) < O(n) < O(n*log n) < O(n^2) < O(2^n) < O(n!)
	
=====================================================================
 4. Examples
=====================================================================