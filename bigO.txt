=====================================================================
 0. Table Of Contents
=====================================================================
 1. Overview
 2. Different Complexities
 3. Complexity Comparison
 4. Examples
 
=====================================================================
 1. Overview
=====================================================================

Big O notation is used to describe efficiency of algorithms
(time and space complexity) in relation to input size.

Big O notation is NOT about an algorithm being "quick" or "slow".
Rather it is about the rate of increase - how fast the time will
increase in proportion to the input size.

 Example
 -------
 The following for-loop has complexity O(n):

	for(i=0; i<n; i++){
		O(1) instruction1;
		O(1) instruction2;
	}

 The following loops are also O(n):
	
	// O(n) + O(n) = O(2n) = O(n)
	for(i=0; i<n; i++){
		O(1) instruction1;
	}
	
	for(i=0; i<n; i++){
		O(1) instruction2;
	}
	
 The single for-loop will run faster then the two separate ones
 because it does n iterations only once. Complexity is the same
 but not the run time.

 Example
 -------
 Insertion sort has complexity O(n^2) whereas quick sort - O(n*log n).
 O(n*log n) complexity is better than O(n^2). Yet, insertion sort is
 faster than quick sort for small input.
 
 Note: there are two sides to learning Big O:
 
	1. Know and understand what different complexities mean
	2. Understand how to calculate Big O for a piece of code

=====================================================================
 2. Different Complexities
=====================================================================

 * O(1) - constant complexity

   No matter how large is the input, it always takes one operation/
   will execute in the same time, e.g.
   
		print x
		x + y
		if (x < 5) { ... }
		
 * O(log n) - logarithmic complexity (assume log base is 2)
 
   If we double the input, only one extra operation/iteration will be
   done.
   
   In general, for O(log n) problems on each iteration we halve the
   dataset (and we end up doing it log n time only).
   
   Let's use binary search to find x=4 in:
   
		{1, 4, 5, 7, 8, 9, 11, 12, 15}
		
		0) start with the full list (n = 9, indices 0 to 8) 
		1) split the list in half (n = 9/2 = 4, indices 0 to 4)
		2) split the list in half (n = 4/2 = 2, indices 0 to 2)
		3) split the list in half (n = 2/2 = 1, indices 0 to 1)
		
	We split n 3 times to find x=4 in the list:
	
		log n = log 9 ~ 3   ==> O(log n)
		
	Note: if we were looking for 8, we would find it after the first
	split so "log 9 ~ 3" would not hold. But with big O in programming
	we look at the worst case which means an element that is at the end
	or beginning of our sample list (like 3).
		
 * O(n) - linear complexity
  
   The number of operations increases proportionally to the size of 
   input, e.g. double the size of input and the number of operations
   will double too.
   
   Good example of O(n) is a simple for-loop:
   
		for(i=0; i<n; i++){
			print i;
		}
	
	Initially, the loop runs n times. Then we increase the size of
	dataset by 5 elements (n+5) and the loop runs 5 more times. The
	increase is proportional to the size of input.
		
 * O(n*log n) - loglinear complexity
 
   This type of complexity usually happens when you need to do work of
   O(log n) for each record (and there is n records).
   
   A good example is merge sort algorithm where we split an unsorted
   list into n sublists, each containing one element. Then repeatedly
   we merge sublists to produce new sorted sublists until there is only
   one sublist remaining:

		O(log n) because we halve the list until we have 1 element lists
		O(n) because of the max number of comparisons on each level
		
 * O(n^2) - squared complexity
 
   The number of operations gets squared in relation to the size of input.
   
   A good example is a nested for-loop:
   
		for(i=0; i<n; i++) {
			for(j=0; j<n; j++) {
				print i,j;
			}
		}
		
	For each element we go through all elements. If n = 2, print will
	be executed four times - twice for i = 0 and twice for i = 1:
	
			0,0
			0,1
			1,0
			1,1
			
	If we change to n = 3, print will be executed... nine times (thrice
	for i = 0, thrice for i = 1 and thrice for i = 2).
	
 * O(2^n) - exponential complexity
 
   Every time an element is added to the input, the number of executions 
   doubles.
   
   A good example here is a recursive version of Fibonacci:
   
		fib(int n) {
			if n <= 2:
				return 1
			return fib(n-1) + fib(n-2)
		}
		
	For n = 2, our execution tree has only one node:
	
					fib(2)
					
	For n = 3, we add an extra level and double the number of executions: 
	
					fib(3)					
				       /      \
				   fib(2)     fib(1)				
				
	For n = 5, we add more two more levels (one not fully complete) and
	the	number of executions in general doubles per level:
	
					fib(5)					
				       /      \
				  fib(4)     fib(3)				
				 /    |      |	  \
		             fib(3) fib(2) fib(2) fib(1)
                             /    \
                        fib(2)	fib(1)	  
			
 * O(n!) - factorial complexity
 
   By adding an extra element to input of size n, the number of
   executions increases n times.
   
   A good example is the travelling salesman problem where we need to
   consider all permutations on n-element dataset (=n!).
	
=====================================================================
 3. Complexity Comparison
=====================================================================

In general, O(1) is the best complexity. All the others compare as
follows:

	O(1) < O(log n) < O(n) < O(n*log n) < O(n^2) < O(2^n) < O(n!)
	
=====================================================================
 4. Examples
=====================================================================

 * Example 1 - O(1)
 
	foo(int array, int idx) {
	  print array[idx];
 	}
   
   It does not matter how many elements we have in array; we print only
   one element. Thus, the complexity is O(1).
   
 * Example 2 - O(n)
 
 	foo(int array) {
	  for(i=0; i<array.size(); i++) {
	    print array[i];
	  }
	}
   
   Printing a single element of array is O(1). However, we repeat this
   operation for each element, thus doing it n times, e.g.
     - for array with 5 elements, print is executed 5 times
     - for array with 6 elements, print is executed 6 times
     
   The number of operations increases proportionally to the input, thus
   complexity is O(n).
 
  * Example 3 - O(n)
  
  	foo(int array) {
	  for(i=0; i<array.size(); i+=2){
	    print array[i];
	  }
	}

   Printing a single element of array is O(1). However, we repeat this
   operation for every other element, e.g.
     - for array with 5 elements, print i in {0, 2, 4}
     - for array with 6 elements, print i in {0, 2, 4}
     - for array with 12 elements, print i in {0, 2, 4, 6, 8, 10}
     - for array with 24 elements, print i in {0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22}
     
   Every time we double the number of elements, we double the number of
   executed print instructions. The number of operations increases
   proportionally to the input, thus complexity is O(n).

  * Example 4 - O(log n)
  
  	foo(int array) {
	  for(i=1; i<array.size(); i*=2) {
	    print array[i];
	  }
	}
	
    Printing a single element of array is O(1). However, we repeat this
    operation for only a number of elements, e.g.
      - for array with 5 elements, print i in {1, 2, 4}
      - for array with 8 elements, print i in {1, 2, 4}
      - for array with 16 elements, print i in {1, 2, 4, 8}
      - for array with 32 elements, print i in {1, 2, 4, 8, 16}
      
    Every time we double the number of elements, we add only one extra
    print instruction. The number of prints equals log N:
    
    	log 8 = 3	log 16 = 4	log 32 = 5
	
    Thus, complexity is O(log n).
    
  * Example 5 - O(n^2)

	foo(int array) {
	  for(i=0; i<array.size(); i++) {
	    for(j=0; j<array.size(); j++) {
	      print array[i] + array[j];
	    }
	  }
	}

    Printing a single value is O(1) and adding two numbers is O(1). Thus,
    the inside of nested for-loop is O(1). However, we repeat it in a nested
    for-loop.
    
    The external for-loop runs n times. The internal for-loop runs also n
    times but it does it for every run of the external loop. E.g. for an
    array with 2 elements for each i, the internal loop is run twice:
        (i = 0, j = 0)
	(i = 0, j = 1)
	(i = 1, j = 0)
	(i = 1, j = 1)
	
    The total number of runs is 4 (2^2). For an array with 3 elements for 
    each i, the internal loop is run thrice:
    	(i = 0, j = 0)
	(i = 0, j = 1)
	(i = 0, j = 2)
	(i = 1, j = 0)
	...
	(i = 2, j = 2)
	
    The total number of executions is 9 (3^2). So the number of executions
    is n^2 and thus, the complexity is O(n^2).
    
  * Example 6 - O(log(log n))
  
  	foo(int array) {
	  for(i = 2; i<array.size(); i=i*i){
	    print array[i];
	  }
	}

    Printing a single element of array is O(1). However, we repeat this only
    a handful of times:
      - for an array with 2 elements, i = {} (0 no runs)
      - for an array with 4 elements, i = {2} (1 run) 
      - for an array with 8 elements, i = {2, 4} (2 runs)
      - for an array with 16 elements, i = {2, 4} (2 runs)
      - for an array with 32 elements, i = {2, 4, 16} (3 runs)
      - for an array with 64 elements, i = {2, 4, 16} (3 runs)
      - for an array with 128 elements, i = {2, 4, 16} (3 runs)
      - for an array with 256 elements, i = {2, 4, 16} (3 runs)
      - for an array with 512 elements, i = {2, 4, 16, 256} (4 runs)
      
    To come up with the complexity, we only need to have a closer look at 
    arrays with 2, 4, 16 and 256 elements:
    	n = 2   = 2^1	= 2^(2^0)	k = 0 (number of runs)
	n = 4   = 2^2	= 2^(2^1)	k = 1
	n = 16  = 2^4	= 2^(2^2)	k = 2
	n = 256 = 2^8	= 2^(2^3)	k = 3

    This gives us:
    	n = 2^(2^k)	=> log n = 2^k	=> log(log n) = k

    Thus, the complexity is log(log n).

  * Example 7 - O(2^n)
  
  	foo(int n){
	  if (n <= 1) {
	    return 1;
	  }
	  return foo(n-1) + foo(n-1);
	}
	
    The if-statements is O(1). However, the big O is driven here by the
    two recursive calls.
    
    If n = 1, there is only one call:
    			foo(1)			1 node
			
    If n = 2, we add an extra level to the execution tree:
    			foo(2)			1 node
			/    \
		    foo(1)   foo(1)		2 nodes
		    
    If n = 3, we add another level:
    			foo(3)			1 node
		       /      \
		 foo(2)        foo(2)		2 nodes
	        /     \       /     \
	   foo(1)   foo(1)  foo(1)  foo(1)	4 nodes
    
    The depth of the execution tree is n and the number of nodes 
    on each level i is 2^i (i = {0, 1, ..., n-1}). Thus, the total number
    of nodes is:
    	n = 1, i = {0} 		=> 2^0
	n = 2, i = {0, 1}	=> 2^0 + 2^1
	n = 3, i = {0, 1, 2}	=> 2^0 + 2^1 + 2^2
	
    Thus, we can express the number of executions as a geometric series:
    	1 * 2^0 + 1 * 2^1 + 1 * 2^2 + ... + 1 * 2^(n-1)
	
    The sum of our geometric series can be expressed as:
	(1 - 2^n)/(1 - 2) = (1 - 2^n) * (-1) = 2^n - 1
	
    Thus, the total number of executions is 2^n and as a result the complexity
    is O(2^n).

  * Example 8 - O(n)
  
  	sum(Node node) {
	  if (node == null) {
	    return 0;
	  }
	  return sum(node.left) + node.val + sum(node.right);
	}
	
   The if-statement has is O(1). However, the big O is driven here by the
   two recursive calls.
   
   For a tree with 1 node, the number of recursive calls i is 2 because we
   need to check the left and right child (which do not exist):
   			()		n = 1, i = 2

   For a tree with 2 nodes, the number of recursive calls i is 4 because we 
   check the left and right child for each node in the tree (even if a child 
   is null):
   			()		n = 2, i = 4
		       /
		      ()
	
   For a tree with 3 nodes, the number of recursive calls i is 6:
   			()		n = 3, i = 6
		       /  \
		     ()    ()

   We can see that the number of recursive calls i = 2 * n. Thus, the complexity
   is O(2*n) which is simply O(n).
   
  * Example 9 - O(a/b)
  
  	foo div(int a, int b) {
	  int count = 0;
	  int sum = b;
	  
	  while(sum <= a) {
	    sum += b;
	    count++;
	  }
	  return count;
	}
	
   The number of times while-loop gets executed depends on the value of a and b:
   	a < b		sum = b + 0 * b		iter = 0
	a = b		sum = b + 1 * b		iter = 1
	a = 2b		sum = b + 2 * b		iter = 2
	a = 3b		sum = b + 3 * b		iter = 3
	
   In other words:
   	a = iter * b	=> iter = a / b
	
   The number of iterations can be expressed as (a / b) and each command within the
   loop (as well as outside of it) is O(1), thus the complexity is O(a/b).

  * Example 10 - O(n^2 * n!)
  
  	void permutation(String str) {
	  permutation(str, "");
	}
	
	void permutation(String str, String prefix) {
	  if(str.length() == 0) {
	    System.out.println(prefix);
	  } else {
	    for(int i=0; i<str.length(); i++) {
	      String rem = str.substring(0, i) + str.substring(i+1);
	      permutation(rem, prefix + str.charAt(i));
	    }
	  }
	}
   
   This is a tricky one. We need to take into account:
     - number of recursive calls
     - complexity of permutation function as a whole (System.out.printl, for loop)
     
   To get the number of recursive calls, let's have a look at what happens to "abcd"
   (note: we are tracking values of rem only, ignoring prefix):
   					abcd
		                /      /    \      \
			     bcd     acd    abd     abc
                       /    /   \   / | \  / | \   / | \ 
                     cd    bd   bc   ...    ...     ...
                    /  \  /  \  / \
		   d    c  ...  ...
                   |    |
	           ""   ""
    
    The depth of tree is 4 which is the same as the length of our string. The number
    of nodes increases for each level:
    	l = 0		nodes = 1	
	l = 1		nodes = 4	(4 * 1)	
	l = 2		nodes = 12 	(4 * 3 * 1)
	l = 3		nodes = 24 	(4 * 3 * 2 * 1) = 4!

    The leaves represent the base calls and, as we can see for n = 4, the total number
    of leaves is n!. To get to a leave we need n recursive calls, thus the total number
    of recursive calls is n * n!
    
    We also need to take into account complexity of System.out.println which is part of
    base call. Since it needs to loop over each character to print a string, the complexity
    is O(n).
    
    Finally, we need to consider the substring calls in the for-loop. In the older versions
    of Java the complexity of substring would be O(1) so taking the for-loop into account,
    the complexity would be O(n). However, in the newer version of Java substring complexity
    changed to O(n), thus the total complexity will be O(n^2).
    
    Adding this all up, we have (n * n!) recursive calls and each does:
      - (2 * n) operations for older Java versions
      - (n + n^2) operations for new Java versions
    
    Thus, the complexity is:
      - O(2 * n * n * n!) = O(n^2 * n!) (older Java)
      - O((n + n^2) * n * n!) = O(n^2 * n * n!) = O(n^3 * n!) (newer Java)
    	
    
    
    
